{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9ba23e",
   "metadata": {},
   "source": [
    "# PProductions — EDA e Modelagem (IMDb)\n",
    "\n",
    "Este notebook executa:\n",
    "1. **EDA** com variáveis numéricas, categóricas e de texto.\n",
    "2. **Insights** sobre fatores de faturamento (Gross).\n",
    "3. **Modelagem** da **nota IMDb** (regressão) e **faturamento** (regressão).\n",
    "4. **Salvamento do modelo** `.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c9d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "\n",
    "DATA_PATH = Path('../data/imdb.csv')  # ajuste se necessário\n",
    "assert DATA_PATH.exists(), f\"Arquivo não encontrado: {DATA_PATH}. Coloque o seu csv em data/imdb.csv\"\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "df = df_raw.copy()\n",
    "print(df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapeamento tolerante de colunas\n",
    "def find_col(df, candidates):\n",
    "    for c in df.columns:\n",
    "        if c.lower().strip() in [x.lower() for x in candidates]:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        for x in candidates:\n",
    "            if x.lower() in c.lower():\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "col_map = {\n",
    "    'title': find_col(df, ['Series_Title','Title','Movie','SeriesTitle']),\n",
    "    'year': find_col(df, ['Released_Year','Year','Release_Year']),\n",
    "    'cert': find_col(df, ['Certificate','Rating','MPAA']),\n",
    "    'runtime': find_col(df, ['Runtime','Runtime_Minutes']),\n",
    "    'genre': find_col(df, ['Genre','Genres']),\n",
    "    'overview': find_col(df, ['Overview','Description','Plot']),\n",
    "    'metascore': find_col(df, ['Meta_score','Metascore']),\n",
    "    'director': find_col(df, ['Director']),\n",
    "    'star1': find_col(df, ['Star1','Actor1','Lead1']),\n",
    "    'star2': find_col(df, ['Star2','Actor2','Lead2']),\n",
    "    'star3': find_col(df, ['Star3','Actor3','Lead3']),\n",
    "    'star4': find_col(df, ['Star4','Actor4','Lead4']),\n",
    "    'votes': find_col(df, ['No_of_Votes','Votes','NumVotes']),\n",
    "    'gross': find_col(df, ['Gross','BoxOffice','Revenue','Worldwide_Gross']),\n",
    "    'imdb': find_col(df, ['IMDB_Rating','IMDb','Rating_IMDb']),\n",
    "}\n",
    "col_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ff5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Limpeza \n",
    "df['runtime_min'] = pd.to_numeric(\n",
    "    df[col_map['runtime']].astype(str).str.extract(r'(\\d+)')[0], errors='coerce'\n",
    ") if col_map['runtime'] else np.nan\n",
    "\n",
    "def parse_money(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).replace(',', '').replace('$','').replace('£','').replace('€','')\n",
    "    try: return float(s)\n",
    "    except: return np.nan\n",
    "\n",
    "df['gross_num'] = df[col_map['gross']].apply(parse_money) if col_map['gross'] else np.nan\n",
    "df['year_num'] = pd.to_numeric(df[col_map['year']], errors='coerce') if col_map['year'] else np.nan\n",
    "df['votes_num'] = pd.to_numeric(df[col_map['votes']], errors='coerce') if col_map['votes'] else np.nan\n",
    "df['metascore_num'] = pd.to_numeric(df[col_map['metascore']], errors='coerce') if col_map['metascore'] else np.nan\n",
    "df['imdb_num'] = pd.to_numeric(df[col_map['imdb']], errors='coerce') if col_map['imdb'] else np.nan\n",
    "\n",
    "df[['runtime_min','gross_num','year_num','votes_num','metascore_num','imdb_num']].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Distribuições\n",
    "for c in ['imdb_num','gross_num','votes_num','metascore_num','runtime_min']:\n",
    "    if c in df:\n",
    "        df[c].plot(kind='hist', bins=30, alpha=0.7, title=f'Distribuição: {c}')\n",
    "        plt.xlabel(c); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63254c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlação (Spearman)\n",
    "corr = df[['imdb_num','gross_num','votes_num','metascore_num','runtime_min','year_num']].corr(method='spearman')\n",
    "corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gêneros\n",
    "if col_map['genre']:\n",
    "    genres = df[col_map['genre']].dropna().astype(str).str.get_dummies(sep=',').rename(columns=lambda x: x.strip())\n",
    "    top_genres = genres.sum().sort_values(ascending=False).head(15)\n",
    "    top_genres.plot(kind='bar', title='Top gêneros'); plt.ylabel('Contagem'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Texto: Overview\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "if col_map['overview']:\n",
    "    text_all = ' '.join(df[col_map['overview']].dropna().astype(str).tolist())\n",
    "    wc = WordCloud(stopwords=STOPWORDS, width=1000, height=500).generate(text_all)\n",
    "    plt.imshow(wc); plt.axis('off'); plt.title('Nuvem de palavras — Overview'); plt.show()\n",
    "\n",
    "    df['overview_len'] = df[col_map['overview']].astype(str).str.len()\n",
    "    df['overview_words'] = df[col_map['overview']].astype(str).str.split().apply(len)\n",
    "    df[['overview_len','overview_words']].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e97f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hipóteses rápidas\n",
    "import pandas as pd\n",
    "hyp = {}\n",
    "for col in ['votes_num','metascore_num','runtime_min','year_num']:\n",
    "    if col in df:\n",
    "        hyp[col] = df[[col,'imdb_num']].corr(method='spearman').iloc[0,1]\n",
    "pd.Series(hyp, name='Spearman vs IMDb').sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modelagem IMDb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "target = 'imdb_num'\n",
    "assert target in df, \"Coluna de alvo IMDb não detectada; ajuste col_map ou nome da coluna no dataset.\"\n",
    "\n",
    "cat_cols = [c for c in [col_map['cert'], col_map['genre'], col_map['director'], col_map['star1'], col_map['star2'], col_map['star3'], col_map['star4']] if c]\n",
    "num_cols = [c for c in ['runtime_min','votes_num','metascore_num','year_num','gross_num'] if c in df]\n",
    "\n",
    "X = df[cat_cols + num_cols].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('num', StandardScaler(with_mean=False), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', min_frequency=10), cat_cols),\n",
    "], remainder='drop')\n",
    "\n",
    "pipe = Pipeline([('prep', preprocess), ('rf', RandomForestRegressor(n_estimators=400, random_state=42))])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "print({'RMSE': rmse, 'MAE': mae})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Salvar .pkl\n",
    "import joblib, os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "path = '../models/model_imdb.pkl'\n",
    "joblib.dump(pipe, path)\n",
    "path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53abeedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fatores de faturamento (Gross) — \n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "if 'gross_num' in df and df['gross_num'].notna().sum() > 50:\n",
    "    y2 = df['gross_num']\n",
    "    X2 = df[cat_cols + num_cols].copy()\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "    pre2 = ColumnTransformer([\n",
    "        ('num', StandardScaler(with_mean=False), [X2.columns.get_loc(c) for c in num_cols]),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', min_frequency=10), [X2.columns.get_loc(c) for c in cat_cols]),\n",
    "    ], remainder='drop')\n",
    "\n",
    "    model_rev = Pipeline([('prep', pre2), ('gbr', GradientBoostingRegressor(random_state=42))])\n",
    "    model_rev.fit(X2_train, y2_train)\n",
    "    y2_pred = model_rev.predict(X2_test)\n",
    "    print('RMSE Gross:', mean_squared_error(y2_test, y2_pred, squared=False))\n",
    "\n",
    "    pi = permutation_importance(model_rev, X2_test, y2_test, n_repeats=5, random_state=42)\n",
    "    importances = pd.Series(pi.importances_mean, index=model_rev.named_steps['prep'].get_feature_names_out())\n",
    "    importances.sort_values(ascending=False).head(20)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
